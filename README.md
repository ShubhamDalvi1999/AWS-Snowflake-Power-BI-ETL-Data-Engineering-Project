<h2 align="center">
  Welcome to My AWS-Based NSE Data Analysis Project!
  <img src="https://media.giphy.com/media/hvRJCLFzcasrR4ia7z/giphy.gif" width="28">
</h2>

<!-- Intro  -->
<h3 align="center">
        <samp>&gt; Hey There!, I am
                <b><a target="_blank" href="https://yourwebsite.com">Shubham Dalvi</a></b>
        </samp>
</h3>

<p align="center"> 
  <samp>
    <br>
    „Äå I am a data engineer with a passion for big data, distributed computing, cloud solutions, and data visualization „Äç
    <br>
    <br>
  </samp>
</p>

<div align="center">
<a href="https://git.io/typing-svg"><img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&pause=1000&random=false&width=435&lines=Spark+%7C+DataBricks+%7C+Power+BI+;Snowflake+%7C+Azure+%7C+AWS;3+yrs+of+IT+experience+as+Analyst+%40+;Accenture+;Passionate+Data+Engineer+" alt="Typing SVG" /></a>
</div>

<p align="center">
 <a href="https://linkedin.com/in/yourprofile" target="_blank">
  <img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white" alt="yourprofile"/>
 </a>
</p>
<br />

<!-- About Section -->
# About Me

<p>
 <img align="right" width="350" src="/assets/programmer.gif" alt="Coding gif" />
  
 ‚úåÔ∏è &emsp; Enjoy solving data problems <br/><br/>
 ‚ù§Ô∏è &emsp; Passionate about big data technologies, cloud platforms, and data visualizations<br/><br/>
 üìß &emsp; Reach me: dshubhamp1999@gmail.com<br/><br/>
</p>

<br/>
<br/>
<br/>

## Skills and Technologies

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PySpark](https://img.shields.io/badge/PySpark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)
![Matplotlib](https://img.shields.io/badge/Matplotlib-013243?style=for-the-badge&logo=matplotlib&logoColor=white)
![Jupyter](https://img.shields.io/badge/Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white)
![AWS](https://img.shields.io/badge/AWS-232F3E?style=for-the-badge&logo=amazon-aws&logoColor=white)
![Git](https://img.shields.io/badge/Git-F05032?style=for-the-badge&logo=git&logoColor=white)
![VSCode](https://img.shields.io/badge/Visual_Studio-0078d7?style=for-the-badge&logo=visual%20studio&logoColor=white)

<br/>

## Project Overview

This project combines the power of **PySpark** with **AWS services** to analyze NSE (National Stock Exchange) data efficiently. It demonstrates how cloud computing can be leveraged for large-scale data processing, storage, and analysis. By integrating AWS technologies such as **S3** and **Glue**, this project facilitates efficient handling of historical futures data to identify trading signals based on changes in open interest and closing prices.

## Table of Contents
- [Technologies Used](#technologies-used)
- [Skills Demonstrated](#skills-demonstrated)
- [AWS Architecture](#aws-architecture)
- [Data Preprocessing](#data-preprocessing)
- [Data Transformation](#data-transformation)
- [Analysis and Filtering](#analysis-and-filtering)
- [Saving Results](#saving-results)
- [Visualizing Data](#visualizing-data)
- [Usage Instructions](#usage-instructions)

## Technologies Used
- **AWS S3**: For storing raw and processed data.
- **AWS Glue**: For orchestrating ETL jobs.
- **PySpark**: For distributed data processing.
- **Pandas**: For additional data manipulation and transformation.
- **Matplotlib**: For data visualization.
- **Jupyter Notebook**: For interactive data analysis.

## Skills Demonstrated
- **Cloud Integration**: Using AWS services for scalable data solutions.
- **Data Engineering**: Efficient handling and processing of large datasets.
- **PySpark**: Advanced usage of PySpark DataFrame operations and SQL functions.
- **Data Transformation**: Converting and cleaning data for analysis.
- **Data Analysis**: Identifying trading signals based on predefined conditions.
- **Visualization**: Plotting data distributions for insights.
- **Performance Optimization**: Using repartitioning and coalescing techniques to manage large datasets.

## AWS Architecture
1. **AWS S3**: Raw data files are uploaded to an S3 bucket. Processed data and final results are also stored here.
2. **AWS Glue**: ETL jobs are configured to clean, transform, and load data into structured formats for analysis.
3. **EC2/Jupyter Notebook**: Data analysis and visualization are performed using a Jupyter Notebook hosted on an AWS EC2 instance.

## Usage Instructions
1. Upload raw NSE data files to the designated S3 bucket.
2. Set up AWS Glue ETL jobs or run the PySpark script on an EC2 instance.
3. Process the data using the Jupyter Notebook or AWS Glue workflow.
4. Analyze the results saved in the S3 bucket or visualize them locally.
